# Month 01: Python & Data Foundations

**Focus**: Build strong foundations in Python programming and data manipulation.

---

## Why It Matters

Python and data manipulation skills are the bedrock of all AI/ML work. Every data science role requires fluency in Python and libraries like Pandas and NumPy. This foundation will be used in every subsequent month.

**Job Relevance**: Required for 100% of data science and ML engineering roles.

---

## Prerequisites

- Basic programming concepts (variables, loops, functions)
- Familiarity with command line
- Git basics

---

## Learning Goals

**Tier 1 Technologies**:
- Python (advanced features, best practices)
- Pandas (data manipulation)
- NumPy (numerical computing)
- Matplotlib, seaborn (visualization)
- Jupyter (interactive development)
- VS Code (editor setup)
- Git/GitHub (version control)

**Skills**:
- Data loading and cleaning
- Exploratory data analysis
- Data visualization
- Version control workflow

---

## Main Project: EDA Portfolio Piece

Build a comprehensive exploratory data analysis project on a real-world dataset.

### Deliverables

1. **Jupyter Notebook** with complete EDA
2. **Data Profile Document** summarizing the dataset
3. **Visualization Dashboard** (Streamlit or Jupyter widgets)
4. **GitHub Repository** with proper README

### Definition of Done

- [ ] Dataset selected and loaded
- [ ] Missing value analysis complete
- [ ] All columns explored (univariate analysis)
- [ ] Correlations and relationships analyzed (bivariate)
- [ ] Data quality issues documented
- [ ] At least 10 visualizations created
- [ ] Key insights summarized
- [ ] Code is clean and documented
- [ ] Repository has proper README
- [ ] Notebook runs end-to-end without errors

---

## Stretch Goals

- [ ] Interactive Streamlit dashboard
- [ ] Automated data profiling with pandas-profiling
- [ ] Compare multiple datasets
- [ ] Add statistical tests for insights

---

## Weekly Cadence

### Week 1: Setup & Data Selection
- Set up Python environment
- Configure VS Code and Jupyter
- Select dataset (Kaggle, UCI, government data)
- Initial data exploration

### Week 2: Deep Dive Analysis
- Apply EDA-to-Insight skill
- Create visualizations
- Document findings

### Week 3: Harden & Document
- Clean up code
- Add docstrings
- Write README
- Create data profile document

### Week 4: Ship & Reflect
- Finalize visualization dashboard
- Record demo
- Write project summary
- Retrospective

---

## Claude Prompts

### Planning
```
/plan-week

I'm starting Month 1, focused on Python and EDA. I have [X] hours this week.
Help me plan the first week with setup and data selection.
```

### Building
```
/ship-mvp

I've completed my initial EDA notebook. Check if I'm ready for MVP:
- Dataset: [name]
- Columns explored: [number]
- Visualizations: [number]
```

### Review
```
/harden

Review my EDA notebook for:
- Code quality
- Visualization clarity
- Documentation completeness
```

### Reflection
```
/retro

Month 1 is complete. Help me reflect on:
- What I learned about Python/Pandas
- What was harder than expected
- What to carry forward
```

---

## How to Publish

### Demo
- Show notebook execution
- Walk through key visualizations
- Highlight interesting findings

### Write-up
- "My First Data Analysis: What I Learned"
- Focus on the analytical process, not just results
- Include code snippets for key techniques

---

## Resources

### Datasets
- [Kaggle Datasets](https://www.kaggle.com/datasets)
- [UCI ML Repository](https://archive.ics.uci.edu/ml/index.php)
- [Data.gov](https://data.gov/)

### Skill Playbook
- [EDA to Insight](../../../.claude/skills/eda-to-insight.md)

### Documentation
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/)
- [Seaborn Tutorial](https://seaborn.pydata.org/tutorial.html)

---

## Next Month Preview

**Month 02**: ML Fundamentals â€” Build your first classification pipeline using scikit-learn.

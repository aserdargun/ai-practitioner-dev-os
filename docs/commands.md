# Commands Guide

Complete guide to all commands available in the learning system.

---

## Quick Reference

| Command | Purpose | Agent |
|---------|---------|-------|
| `/status` | Check progress | Evaluator |
| `/plan-week` | Generate weekly plan | Planner |
| `/start-week` | Begin the week | Planner |
| `/ship-mvp` | Ship working code | Builder |
| `/harden` | Add tests and docs | Builder |
| `/publish` | Prepare for demo | Builder |
| `/retro` | Weekly retrospective | Coach |
| `/evaluate` | Score progress | Evaluator |
| `/adapt-path` | Propose changes | Evaluator |
| `/add-best-practice` | Capture learning | Coach |
| `/debug-learning` | Get unstuck | Coach |
| `/report` | Update tracker | Evaluator |

---

## Detailed Command Reference

### `/status`

**Purpose**: Check your current progress and identify blockers.

**Agent**: Evaluator

**When to use**: Daily check-in, before starting work

**What it shows**:
- Current week and month
- Tasks in progress
- Completed tasks
- Any blockers or warnings

**Example**:
```
/status

ğŸ“Š Status Report
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Month: 02 (Data Engineering)
Week: 3 of 4

Progress:
âœ… Completed: 4 tasks
ğŸ”„ In Progress: Data validation pipeline
â³ Remaining: 2 tasks

Status: On Track (78%)
```

---

### `/plan-week`

**Purpose**: Generate this week's task list based on current month objectives.

**Agent**: Planner (with Coach support)

**When to use**: Start of each week (Monday)

**What it produces**:
- Task list for the week
- Estimated effort per task
- Resources and references
- Journal entry in `paths/Advanced/journal/`

**Example**:
```
/plan-week

ğŸ“… Week 3 Plan Generated
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Focus: Data Validation & Quality

Tasks:
1. [ ] Implement Pydantic models (2h)
2. [ ] Write validation tests (2h)
3. [ ] Create data pipeline (3h)
4. [ ] Document the API (1h)

See: paths/Advanced/journal/week-03.md
```

---

### `/start-week`

**Purpose**: Initialize the week and run pre-week checks.

**Agent**: Planner

**When to use**: Monday morning

**What it does**:
1. Runs `pre_week_start.sh` hook
2. Validates environment
3. Checks for uncommitted work
4. Shows week overview

**Example**:
```
/start-week

ğŸš€ Starting Week 3
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Running pre-week checks...

âœ“ Git repository clean
âœ“ Python 3.11 detected
âœ“ Memory files present
âœ“ No linting errors

Ready to begin!
Run /plan-week if you haven't already.
```

---

### `/ship-mvp`

**Purpose**: Ship the minimum viable version of your current task.

**Agent**: Builder (with Reviewer support)

**When to use**: When feature is ready for first working version

**What it does**:
1. Reviews current task requirements
2. Identifies applicable template
3. Helps implement core functionality
4. Creates commit

**Example**:
```
/ship-mvp

ğŸš€ Shipping MVP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Task: Data validation pipeline

Actions:
âœ“ Created src/pipeline/validator.py
âœ“ Added Pydantic models
âœ“ Basic tests passing (3/3)

Commit: abc1234
"Add data validation pipeline with Pydantic models"
```

---

### `/harden`

**Purpose**: Add tests, documentation, and error handling to existing code.

**Agent**: Builder (with Reviewer support)

**When to use**: Friday, before publish, or when code needs polish

**What it does**:
1. Reviews existing code
2. Identifies missing tests
3. Adds error handling
4. Updates documentation
5. Runs linting

**Example**:
```
/harden

ğŸ”§ Hardening Code
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Analyzing: src/pipeline/

Improvements:
âœ“ Added 5 edge case tests
âœ“ Added input validation
âœ“ Updated docstrings
âœ“ Fixed 2 linting issues

Test Coverage: 85% (+12%)
```

---

### `/publish`

**Purpose**: Prepare code for demo or portfolio inclusion.

**Agent**: Builder (with Coach support)

**When to use**: End of month, before demo

**What it does**:
1. Runs `pre_publish_check.sh`
2. Verifies all tests pass
3. Checks documentation completeness
4. Creates demo-ready package

**Example**:
```
/publish

ğŸ“¦ Publish Check
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Running quality gates...

Git Status: âœ“ Clean
Linting: âœ“ No errors
Tests: âœ“ 24/24 passing
Docs: âœ“ README complete
Security: âœ“ No secrets found

âœ… Ready to publish!
```

---

### `/retro`

**Purpose**: Conduct weekly retrospective.

**Agent**: Coach (with Evaluator support)

**When to use**: End of week (Friday/Weekend)

**What it does**:
1. Runs `post_week_review.sh`
2. Collects metrics
3. Facilitates reflection
4. Captures action items

**Example**:
```
/retro

ğŸ“ Week 3 Retrospective
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

What Went Well:
â€¢ Completed pipeline ahead of schedule
â€¢ Tests caught 2 bugs early

What Could Improve:
â€¢ Started documentation too late
â€¢ Forgot to log one task completion

Key Learning:
â†’ "Test early, document as you go"
  Added to best_practices.md

Action Items:
[ ] Set daily doc reminder
[ ] Review test coverage reports
```

---

### `/evaluate`

**Purpose**: Run full evaluation of your progress.

**Agent**: Evaluator

**When to use**: End of week, end of month

**What it does**:
1. Reads all memory files
2. Collects repo signals
3. Calculates dimension scores
4. Generates recommendations

**Example**:
```
/evaluate

ğŸ“Š Evaluation Report
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Scores:
Completion   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 82%
Quality      [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘] 75%
Consistency  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘] 88%
Growth       [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘] 70%
Engagement   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 80%

Overall: 79% (On Track)

Recommendations:
â€¢ Add more tests to improve quality
â€¢ Capture learnings as best practices
```

---

### `/adapt-path`

**Purpose**: Get recommendations for learning path changes.

**Agent**: Evaluator (with Coach support)

**When to use**: After evaluation, when struggling, or exceeding expectations

**What it does**:
1. Analyzes evaluation results
2. Applies adaptation rules
3. Proposes mutations
4. Logs decisions

**Example**:
```
/adapt-path

ğŸ”„ Path Analysis
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Current: Month 02, Week 3
Score: 79% (On Track)

Proposed Changes: None

Your current pace is appropriate.
Continue with the planned curriculum.
```

Or with changes:
```
/adapt-path

ğŸ”„ Path Analysis
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Current: Month 02, Week 3
Score: 52% (Needs Attention)

Proposed Changes:

1. [REMEDIATION_WEEK] âš ï¸ Requires Approval
   Insert catch-up week before Month 03
   Focus: Quality improvement, test coverage

Review and approve? (y/n)
```

---

### `/add-best-practice`

**Purpose**: Capture a learning or pattern you've discovered.

**Agent**: Coach

**When to use**: After learning something valuable

**Syntax**: `/add-best-practice "Your learning here"`

**What it does**:
- Appends entry to `best_practices.md`
- Formats with date and context
- Updates growth score

**Example**:
```
/add-best-practice "Always validate data at API boundaries using Pydantic"

âœ… Best Practice Added
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

### 2026-01-15 - Always validate data at API boundaries using Pydantic

Added to: .claude/memory/best_practices.md
```

---

### `/debug-learning`

**Purpose**: Get help when you're stuck on something.

**Agent**: Coach (with Researcher support)

**When to use**: When blocked, confused, or need guidance

**Syntax**: `/debug-learning "Description of your blocker"`

**What it does**:
1. Analyzes the blocker type
2. Provides targeted help
3. Suggests resources
4. Offers next steps

**Example**:
```
/debug-learning "Can't figure out how to mock async functions in pytest"

ğŸ” Debugging: Async Test Mocking
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

The Issue:
Async functions need special handling in pytest.

Solution:
1. Install pytest-asyncio
2. Mark tests with @pytest.mark.asyncio
3. Use AsyncMock from unittest.mock

Example:
```python
import pytest
from unittest.mock import AsyncMock

@pytest.mark.asyncio
async def test_async_function():
    mock = AsyncMock(return_value=42)
    result = await mock()
    assert result == 42
```

Resources:
â€¢ pytest-asyncio docs
â€¢ Real Python async testing guide
```

---

### `/report`

**Purpose**: Generate progress report and update tracker.

**Agent**: Evaluator

**When to use**: Any time you want a summary

**What it does**:
1. Compiles progress data
2. Generates formatted report
3. Updates `tracker.md`

**Example**:
```
/report

ğŸ“ˆ Progress Report Generated
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Month 2, Week 3
Overall: 79%
Status: On Track

Tracker updated: paths/Advanced/tracker.md
```

---

## Command Tips

### Combining Commands

Commands can be run in sequence:
```
/start-week
/plan-week
```

### Getting Help

Ask Claude to explain any command:
```
"What does /harden do exactly?"
```

### Custom Usage

You can ask for variations:
```
"Run /evaluate but focus on quality scores"
```

---

## See Also

- [How to Use](how-to-use.md) â€” Complete workflow guide
- [Agents](agents.md) â€” Agent role details
- [Evaluation Rubric](evaluation/rubric.md) â€” Scoring details

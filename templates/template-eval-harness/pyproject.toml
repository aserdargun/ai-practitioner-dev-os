[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "eval-harness"
version = "0.1.0"
description = "LLM and ML evaluation harness with customizable graders"
readme = "README.md"
requires-python = ">=3.10"
dependencies = []

[project.optional-dependencies]
openai = [
    "openai>=1.0.0,<2.0.0",
]
dev = [
    "pytest>=7.4.0,<9.0.0",
    "pytest-cov>=4.1.0,<6.0.0",
    "ruff>=0.1.0,<1.0.0",
]
all = [
    "eval-harness[openai,dev]",
]

[tool.setuptools.packages.find]
where = ["."]

[tool.ruff]
target-version = "py310"
line-length = 88
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # Pyflakes
    "I",   # isort
    "B",   # flake8-bugbear
    "C4",  # flake8-comprehensions
    "UP",  # pyupgrade
]
ignore = [
    "E501",  # line too long
]

[tool.ruff.isort]
known-first-party = ["evals"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_functions = ["test_*"]
addopts = "-v --tb=short"
filterwarnings = [
    "ignore::DeprecationWarning",
]
